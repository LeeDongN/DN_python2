{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.applications import ResNet50V2\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'D:\\dataset\\dental\\data'\n",
    "\n",
    "data = np.load(base_dir + \"\\\\data_ff.npy\")\n",
    "label = np.load(base_dir + \"\\\\labels_angle_class_face.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_base = ResNet50V2(weights = 'imagenet',\n",
    "                       #classfier를 삭제함!\n",
    "                 include_top = False,\n",
    "                 input_shape = (299, 299, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model3 = conv_base를 4개 동결 해제 시킨 후 학습시킨 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50v2 (Model)           (None, 10, 10, 2048)      23564800  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 204800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               26214528  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 49,779,715\n",
      "Trainable params: 49,734,275\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#메모리 사용량은 batch size와 모델의 복잡성\n",
    "model3 = models.Sequential()\n",
    "model3.add(conv_base)\n",
    "model3.add(layers.Flatten(input_shape=(10,10,2048)))\n",
    "model3.add(layers.Dense(128, activation='relu', input_dim=(10*10*2048)))\n",
    "model3.add(layers.Dense(3, activation='softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 305, 305, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 150, 150, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 75, 75, 64)   256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 75, 75, 64)   0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 75, 75, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 38, 38, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 38, 38, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 38, 38, 256)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 38, 38, 256)  0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 38, 38, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 19, 19, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 19, 19, 512)  0           conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 19, 19, 512)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 19, 19, 512)  0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 10, 10, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 10, 10, 1024) 0           conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 10, 10, 1024) 0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 10, 10, 1024) 0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 10, 10, 2048) 8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 10, 10, 2048) 0           post_bn[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,564,800\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'conv5_block3_3_conv' 가 나오기 전까지 동결됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'conv5_block3_3_conv':\n",
    "         set_trainable = True\n",
    "    layer.trainable = set_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv_base.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data3, test_data3, train_label3, test_label3= train_test_split(data, label, test_size = 0.2, random_state = 123)\n",
    "train_data3, val_data3, train_label3, val_label3 = train_test_split(train_data3, train_label3, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원 핫 인코딩을 위해 label을 바꿈\n",
    "one_hot_train_labels = to_categorical(train_label3 - 1)\n",
    "one_hot_validation_labels = to_categorical(val_label3 - 1)\n",
    "one_hot_test_labels = to_categorical(test_label3 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer=optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\idong\\Anaconda3\\envs\\tens_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 183 samples, validate on 46 samples\n",
      "Epoch 1/150\n",
      "183/183 [==============================] - 10s 56ms/step - loss: 20.4611 - acc: 0.3825 - val_loss: 1.8331 - val_acc: 0.3261\n",
      "Epoch 2/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.1750 - acc: 0.7705 - val_loss: 1.6331 - val_acc: 0.3696\n",
      "Epoch 3/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.3049 - acc: 0.8962 - val_loss: 1.5876 - val_acc: 0.3478\n",
      "Epoch 4/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0696 - acc: 0.9727 - val_loss: 2.4562 - val_acc: 0.3913\n",
      "Epoch 5/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0468 - acc: 0.9781 - val_loss: 2.3418 - val_acc: 0.3913\n",
      "Epoch 6/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 2.0173 - val_acc: 0.3913\n",
      "Epoch 7/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 1.7975 - val_acc: 0.4130\n",
      "Epoch 8/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 1.6945 - val_acc: 0.3696\n",
      "Epoch 9/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 1.6436 - val_acc: 0.3696\n",
      "Epoch 10/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 1.6788 - val_acc: 0.3261\n",
      "Epoch 11/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 1.7821 - val_acc: 0.2609\n",
      "Epoch 12/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.8638 - val_acc: 0.2609\n",
      "Epoch 13/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.9243 - val_acc: 0.2826\n",
      "Epoch 14/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.9452 - val_acc: 0.3043\n",
      "Epoch 15/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.9570 - val_acc: 0.3261\n",
      "Epoch 16/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 8.7452e-04 - acc: 1.0000 - val_loss: 1.9619 - val_acc: 0.3478\n",
      "Epoch 17/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 7.1376e-04 - acc: 1.0000 - val_loss: 1.9725 - val_acc: 0.3478\n",
      "Epoch 18/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 6.0843e-04 - acc: 1.0000 - val_loss: 2.0034 - val_acc: 0.3478\n",
      "Epoch 19/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 5.1645e-04 - acc: 1.0000 - val_loss: 2.0253 - val_acc: 0.3261\n",
      "Epoch 20/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 5.1429e-04 - acc: 1.0000 - val_loss: 2.0505 - val_acc: 0.3043\n",
      "Epoch 21/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.8709e-04 - acc: 1.0000 - val_loss: 2.0669 - val_acc: 0.3043\n",
      "Epoch 22/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.1205e-04 - acc: 1.0000 - val_loss: 2.0708 - val_acc: 0.3043\n",
      "Epoch 23/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 4.2090e-04 - acc: 1.0000 - val_loss: 2.0798 - val_acc: 0.2826\n",
      "Epoch 24/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.1180e-04 - acc: 1.0000 - val_loss: 2.0936 - val_acc: 0.3261\n",
      "Epoch 25/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.3223e-04 - acc: 1.0000 - val_loss: 2.1025 - val_acc: 0.3261\n",
      "Epoch 26/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.0703e-04 - acc: 1.0000 - val_loss: 2.1141 - val_acc: 0.3261\n",
      "Epoch 27/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.7372e-04 - acc: 1.0000 - val_loss: 2.1300 - val_acc: 0.3478\n",
      "Epoch 28/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.5359e-04 - acc: 1.0000 - val_loss: 2.1453 - val_acc: 0.3478\n",
      "Epoch 29/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.0072e-04 - acc: 1.0000 - val_loss: 2.1567 - val_acc: 0.3478\n",
      "Epoch 30/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.3411e-04 - acc: 1.0000 - val_loss: 2.1744 - val_acc: 0.3478\n",
      "Epoch 31/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.1191e-04 - acc: 1.0000 - val_loss: 2.1913 - val_acc: 0.3261\n",
      "Epoch 32/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.0558e-04 - acc: 1.0000 - val_loss: 2.2106 - val_acc: 0.3261\n",
      "Epoch 33/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.2728e-04 - acc: 1.0000 - val_loss: 2.2307 - val_acc: 0.3261\n",
      "Epoch 34/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.9496e-04 - acc: 1.0000 - val_loss: 2.2463 - val_acc: 0.3696\n",
      "Epoch 35/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 1.8237e-04 - acc: 1.0000 - val_loss: 2.2631 - val_acc: 0.3696\n",
      "Epoch 36/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 1.5794e-04 - acc: 1.0000 - val_loss: 2.2867 - val_acc: 0.3696\n",
      "Epoch 37/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 1.4025e-04 - acc: 1.0000 - val_loss: 2.3203 - val_acc: 0.3696\n",
      "Epoch 38/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 1.2370e-04 - acc: 1.0000 - val_loss: 2.3546 - val_acc: 0.3478\n",
      "Epoch 39/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 9.5263e-05 - acc: 1.0000 - val_loss: 2.3884 - val_acc: 0.3478\n",
      "Epoch 40/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 9.7574e-05 - acc: 1.0000 - val_loss: 2.4150 - val_acc: 0.3478\n",
      "Epoch 41/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.6717e-05 - acc: 1.0000 - val_loss: 2.4356 - val_acc: 0.3261\n",
      "Epoch 42/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 7.7464e-05 - acc: 1.0000 - val_loss: 2.4592 - val_acc: 0.3478\n",
      "Epoch 43/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 6.0780e-05 - acc: 1.0000 - val_loss: 2.4960 - val_acc: 0.3261\n",
      "Epoch 44/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.5624e-05 - acc: 1.0000 - val_loss: 2.5212 - val_acc: 0.3043\n",
      "Epoch 45/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.4131e-05 - acc: 1.0000 - val_loss: 2.5269 - val_acc: 0.3043\n",
      "Epoch 46/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 6.9591e-05 - acc: 1.0000 - val_loss: 2.5446 - val_acc: 0.3043\n",
      "Epoch 47/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 5.3254e-05 - acc: 1.0000 - val_loss: 2.5675 - val_acc: 0.3261\n",
      "Epoch 48/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.2412e-05 - acc: 1.0000 - val_loss: 2.5906 - val_acc: 0.3043\n",
      "Epoch 49/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.1374e-05 - acc: 1.0000 - val_loss: 2.6136 - val_acc: 0.2826\n",
      "Epoch 50/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.4875e-05 - acc: 1.0000 - val_loss: 2.6323 - val_acc: 0.3261\n",
      "Epoch 51/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.3426e-05 - acc: 1.0000 - val_loss: 2.6544 - val_acc: 0.3261\n",
      "Epoch 52/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.3493e-05 - acc: 1.0000 - val_loss: 2.6763 - val_acc: 0.3261\n",
      "Epoch 53/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.6179e-05 - acc: 1.0000 - val_loss: 2.6942 - val_acc: 0.3261\n",
      "Epoch 54/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.6346e-05 - acc: 1.0000 - val_loss: 2.7137 - val_acc: 0.3261\n",
      "Epoch 55/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.2548e-05 - acc: 1.0000 - val_loss: 2.7303 - val_acc: 0.3478\n",
      "Epoch 56/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.9511e-05 - acc: 1.0000 - val_loss: 2.7500 - val_acc: 0.3478\n",
      "Epoch 57/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.5046e-05 - acc: 1.0000 - val_loss: 2.7679 - val_acc: 0.3478\n",
      "Epoch 58/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.1344e-05 - acc: 1.0000 - val_loss: 2.7834 - val_acc: 0.3478\n",
      "Epoch 59/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.8726e-05 - acc: 1.0000 - val_loss: 2.7966 - val_acc: 0.3478\n",
      "Epoch 60/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.7716e-05 - acc: 1.0000 - val_loss: 2.8107 - val_acc: 0.3478\n",
      "Epoch 61/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.2839e-05 - acc: 1.0000 - val_loss: 2.8238 - val_acc: 0.3696\n",
      "Epoch 62/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.4859e-05 - acc: 1.0000 - val_loss: 2.8370 - val_acc: 0.3696\n",
      "Epoch 63/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.4340e-05 - acc: 1.0000 - val_loss: 2.8478 - val_acc: 0.3696\n",
      "Epoch 64/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.3909e-05 - acc: 1.0000 - val_loss: 2.8606 - val_acc: 0.3696\n",
      "Epoch 65/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.4456e-05 - acc: 1.0000 - val_loss: 2.8708 - val_acc: 0.3696\n",
      "Epoch 66/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.0868e-05 - acc: 1.0000 - val_loss: 2.8805 - val_acc: 0.3696\n",
      "Epoch 67/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.8836e-05 - acc: 1.0000 - val_loss: 2.8913 - val_acc: 0.3696\n",
      "Epoch 68/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.8035e-05 - acc: 1.0000 - val_loss: 2.9051 - val_acc: 0.3696\n",
      "Epoch 69/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.2225e-05 - acc: 1.0000 - val_loss: 2.9145 - val_acc: 0.3696\n",
      "Epoch 70/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.9128e-05 - acc: 1.0000 - val_loss: 2.9225 - val_acc: 0.3696\n",
      "Epoch 71/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.8187e-05 - acc: 1.0000 - val_loss: 2.9286 - val_acc: 0.3696\n",
      "Epoch 72/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.5871e-05 - acc: 1.0000 - val_loss: 2.9428 - val_acc: 0.3696\n",
      "Epoch 73/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.8300e-05 - acc: 1.0000 - val_loss: 2.9524 - val_acc: 0.3696\n",
      "Epoch 74/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.3991e-05 - acc: 1.0000 - val_loss: 2.9599 - val_acc: 0.3696\n",
      "Epoch 75/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.4298e-05 - acc: 1.0000 - val_loss: 2.9707 - val_acc: 0.3696\n",
      "Epoch 76/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.4304e-05 - acc: 1.0000 - val_loss: 2.9789 - val_acc: 0.3696\n",
      "Epoch 77/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.3588e-05 - acc: 1.0000 - val_loss: 2.9879 - val_acc: 0.3696\n",
      "Epoch 78/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 1.2169e-05 - acc: 1.0000 - val_loss: 2.9921 - val_acc: 0.3696\n",
      "Epoch 79/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.3883e-05 - acc: 1.0000 - val_loss: 2.9970 - val_acc: 0.3696\n",
      "Epoch 80/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.2120e-05 - acc: 1.0000 - val_loss: 3.0021 - val_acc: 0.3696\n",
      "Epoch 81/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.3856e-05 - acc: 1.0000 - val_loss: 3.0081 - val_acc: 0.3696\n",
      "Epoch 82/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.2459e-05 - acc: 1.0000 - val_loss: 3.0114 - val_acc: 0.3696\n",
      "Epoch 83/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.2791e-05 - acc: 1.0000 - val_loss: 3.0175 - val_acc: 0.3696\n",
      "Epoch 84/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 9.6555e-06 - acc: 1.0000 - val_loss: 3.0226 - val_acc: 0.3696\n",
      "Epoch 85/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.1652e-05 - acc: 1.0000 - val_loss: 3.0252 - val_acc: 0.3696\n",
      "Epoch 86/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 9.3708e-06 - acc: 1.0000 - val_loss: 3.0280 - val_acc: 0.3696\n",
      "Epoch 87/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.4940e-05 - acc: 1.0000 - val_loss: 3.0374 - val_acc: 0.3696\n",
      "Epoch 88/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.2289e-05 - acc: 1.0000 - val_loss: 3.0619 - val_acc: 0.3478\n",
      "Epoch 89/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 9.1167e-06 - acc: 1.0000 - val_loss: 3.0789 - val_acc: 0.3478\n",
      "Epoch 90/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.0063e-05 - acc: 1.0000 - val_loss: 3.0908 - val_acc: 0.3478\n",
      "Epoch 91/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.3768e-06 - acc: 1.0000 - val_loss: 3.1012 - val_acc: 0.3478\n",
      "Epoch 92/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.3089e-06 - acc: 1.0000 - val_loss: 3.1090 - val_acc: 0.3478\n",
      "Epoch 93/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 8.7708e-06 - acc: 1.0000 - val_loss: 3.1139 - val_acc: 0.3478\n",
      "Epoch 94/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.9597e-06 - acc: 1.0000 - val_loss: 3.1155 - val_acc: 0.3478\n",
      "Epoch 95/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.2524e-06 - acc: 1.0000 - val_loss: 3.1228 - val_acc: 0.3478\n",
      "Epoch 96/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.7956e-06 - acc: 1.0000 - val_loss: 3.1305 - val_acc: 0.3478\n",
      "Epoch 97/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 7.6740e-06 - acc: 1.0000 - val_loss: 3.1351 - val_acc: 0.3478\n",
      "Epoch 98/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.9565e-06 - acc: 1.0000 - val_loss: 3.1399 - val_acc: 0.3478\n",
      "Epoch 99/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.3442e-06 - acc: 1.0000 - val_loss: 3.1461 - val_acc: 0.3478\n",
      "Epoch 100/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.1605e-06 - acc: 1.0000 - val_loss: 3.1514 - val_acc: 0.3478\n",
      "Epoch 101/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 7.2871e-06 - acc: 1.0000 - val_loss: 3.1551 - val_acc: 0.3478\n",
      "Epoch 102/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.1449e-06 - acc: 1.0000 - val_loss: 3.1615 - val_acc: 0.3478\n",
      "Epoch 103/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 7.5984e-06 - acc: 1.0000 - val_loss: 3.1660 - val_acc: 0.3478\n",
      "Epoch 104/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.2282e-06 - acc: 1.0000 - val_loss: 3.1718 - val_acc: 0.3478\n",
      "Epoch 105/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 1.1059e-05 - acc: 1.0000 - val_loss: 3.1718 - val_acc: 0.3478\n",
      "Epoch 106/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 6.5979e-06 - acc: 1.0000 - val_loss: 3.1622 - val_acc: 0.3478\n",
      "Epoch 107/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.0542e-06 - acc: 1.0000 - val_loss: 3.1643 - val_acc: 0.3478\n",
      "Epoch 108/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 5.8749e-06 - acc: 1.0000 - val_loss: 3.1716 - val_acc: 0.3478\n",
      "Epoch 109/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.6033e-06 - acc: 1.0000 - val_loss: 3.1768 - val_acc: 0.3478\n",
      "Epoch 110/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 7.2010e-06 - acc: 1.0000 - val_loss: 3.1822 - val_acc: 0.3478\n",
      "Epoch 111/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.2795e-06 - acc: 1.0000 - val_loss: 3.1850 - val_acc: 0.3478\n",
      "Epoch 112/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 5.6059e-06 - acc: 1.0000 - val_loss: 3.1864 - val_acc: 0.3478\n",
      "Epoch 113/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.3402e-06 - acc: 1.0000 - val_loss: 3.1894 - val_acc: 0.3478\n",
      "Epoch 114/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.3466e-06 - acc: 1.0000 - val_loss: 3.1944 - val_acc: 0.3478\n",
      "Epoch 115/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.5948e-06 - acc: 1.0000 - val_loss: 3.1993 - val_acc: 0.3478\n",
      "Epoch 116/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.7982e-06 - acc: 1.0000 - val_loss: 3.2012 - val_acc: 0.3478\n",
      "Epoch 117/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.5811e-06 - acc: 1.0000 - val_loss: 3.2027 - val_acc: 0.3478\n",
      "Epoch 118/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.2654e-06 - acc: 1.0000 - val_loss: 3.2045 - val_acc: 0.3478\n",
      "Epoch 119/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.2164e-06 - acc: 1.0000 - val_loss: 3.2096 - val_acc: 0.3478\n",
      "Epoch 120/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.2620e-06 - acc: 1.0000 - val_loss: 3.2158 - val_acc: 0.3478\n",
      "Epoch 121/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.4814e-06 - acc: 1.0000 - val_loss: 3.2237 - val_acc: 0.3478\n",
      "Epoch 122/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.2255e-06 - acc: 1.0000 - val_loss: 3.2360 - val_acc: 0.3478\n",
      "Epoch 123/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 8.4315e-06 - acc: 1.0000 - val_loss: 3.2454 - val_acc: 0.3478\n",
      "Epoch 124/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.1740e-06 - acc: 1.0000 - val_loss: 3.2613 - val_acc: 0.3478\n",
      "Epoch 125/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.4457e-06 - acc: 1.0000 - val_loss: 3.2667 - val_acc: 0.3478\n",
      "Epoch 126/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.6049e-06 - acc: 1.0000 - val_loss: 3.2685 - val_acc: 0.3478\n",
      "Epoch 127/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 4.1768e-06 - acc: 1.0000 - val_loss: 3.2732 - val_acc: 0.3478\n",
      "Epoch 128/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.7459e-06 - acc: 1.0000 - val_loss: 3.2813 - val_acc: 0.3478\n",
      "Epoch 129/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.8407e-06 - acc: 1.0000 - val_loss: 3.2878 - val_acc: 0.3478\n",
      "Epoch 130/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 4.2784e-06 - acc: 1.0000 - val_loss: 3.2910 - val_acc: 0.3478\n",
      "Epoch 131/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.7422e-06 - acc: 1.0000 - val_loss: 3.2881 - val_acc: 0.3478\n",
      "Epoch 132/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.4088e-06 - acc: 1.0000 - val_loss: 3.2890 - val_acc: 0.3478\n",
      "Epoch 133/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 5.3531e-06 - acc: 1.0000 - val_loss: 3.2929 - val_acc: 0.3478\n",
      "Epoch 134/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 4.8040e-06 - acc: 1.0000 - val_loss: 3.2971 - val_acc: 0.3478\n",
      "Epoch 135/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.4941e-06 - acc: 1.0000 - val_loss: 3.3004 - val_acc: 0.3478\n",
      "Epoch 136/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 4.5500e-06 - acc: 1.0000 - val_loss: 3.3042 - val_acc: 0.3478\n",
      "Epoch 137/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 3.1724e-06 - acc: 1.0000 - val_loss: 3.3074 - val_acc: 0.3478\n",
      "Epoch 138/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.2010e-06 - acc: 1.0000 - val_loss: 3.3095 - val_acc: 0.3478\n",
      "Epoch 139/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.0316e-06 - acc: 1.0000 - val_loss: 3.3131 - val_acc: 0.3478\n",
      "Epoch 140/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.5952e-06 - acc: 1.0000 - val_loss: 3.3214 - val_acc: 0.3478\n",
      "Epoch 141/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.7677e-06 - acc: 1.0000 - val_loss: 3.3259 - val_acc: 0.3478\n",
      "Epoch 142/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.1333e-06 - acc: 1.0000 - val_loss: 3.3296 - val_acc: 0.3478\n",
      "Epoch 143/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 3.4882e-06 - acc: 1.0000 - val_loss: 3.3305 - val_acc: 0.3478\n",
      "Epoch 144/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.6695e-06 - acc: 1.0000 - val_loss: 3.3320 - val_acc: 0.3478\n",
      "Epoch 145/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.6662e-06 - acc: 1.0000 - val_loss: 3.3330 - val_acc: 0.3478\n",
      "Epoch 146/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.9847e-06 - acc: 1.0000 - val_loss: 3.3342 - val_acc: 0.3478\n",
      "Epoch 147/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.9469e-06 - acc: 1.0000 - val_loss: 3.3382 - val_acc: 0.3478\n",
      "Epoch 148/150\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 2.5887e-06 - acc: 1.0000 - val_loss: 3.3411 - val_acc: 0.3478\n",
      "Epoch 149/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.5985e-06 - acc: 1.0000 - val_loss: 3.3442 - val_acc: 0.3478\n",
      "Epoch 150/150\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 2.5822e-06 - acc: 1.0000 - val_loss: 3.3467 - val_acc: 0.3478\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model3.fit(train_data3, one_hot_train_labels,\n",
    "                    epochs=150,\n",
    "                    batch_size=40, \n",
    "                    validation_data=(val_data3, one_hot_validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_h = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs_h, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs_h, val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs_h, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs_h, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
